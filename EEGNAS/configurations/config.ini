[DEFAULT]
exp_type = ["per_subject"]
log_epochs = [false]
data_folder = ["data/"]
valid_set_fraction = [0.2]
batch_norm_alpha = [0.1]
dropout_p = [0.5]
max_epochs = [50]
max_increase_epochs = [3]
final_max_epochs = [800]
final_max_increase_epochs = [80]
batch_size = [60]
pin_memory = [false]
do_early_stop = [true]
remember_best = [true]
input_time_cropping = [1000]
n_preds_per_input = [604]
final_conv_size = [2]
channel_dim = ["channels"]
time_factor = [-1]
pop_size = [100]
unique_model_threshold = [0.7]
sim_count = [100]
num_generations = [75]
breed_rate = [1]
breed_rate_deap = [0.3]
num_layers = [10]
dynamic_mutation_rate = [true]
mutation_rate = [0.1]
mutation_rate_increase_rate = [2]
mutation_rate_decrease_rate = [2]
filter_num_max = [50]
kernel_height_max = [20]
kernel_width_max = [1]
conv_stride_max = [1]
pool_height_max = [3]
pool_width_max = [1]
pool_stride_height_max = [3]
pool_stride_width_max = [1]
inherit_weights_normal = [true]
inherit_weights_crossover = [true]
cross_subject_compensation_rate= [1]
cross_subject_sampling_method = ["generation"]
use_aging = [false]
dropout_injection_rate = [0.2]
failed_breedings = [0]
total_breedings = [0]
decay_function = ["linear"]
fitness_function = ["normal_fitness"]
final_test_iterations = [10]
mutation_method = ["mutate_models"]
low_cut_hz = [0]
use_tensorboard = [false]
mutations = [["swap_random_layer"]]
ensembling_method = ["manual"]
mongodb_name = ["EEGNAS"]
mongodb_server = ["132.72.80.67"]
pure_cross_subject = [true]

;-------------------------------------EXPERIMENT CONFIGURATIONS---------------------------------------------------------
[default_exp]
;only purpose of this is to create a default environment for misc. purposes
iterations = [1]

[from_file_one_on_all]
dataset = ["NER15"]
exp_type = ["from_file"]
models_dir = ["456_x_grid_as_linear_NER15"]
model_file_name = ["best_model_iteration_1.th", "best_model_iteration_2.th", "best_model_iteration_3.th", "best_model_iteration_4.th"]
iterations = [1]
final_max_epochs = [100]
final_max_increase_epochs = [10]

[tests]
dataset = ["BCI_IV_2a"]
subjects_to_check=[[1]]
pop_size = [5]
num_generations = [3]
max_epochs = [1]
final_max_epochs = [1]
max_increase_epochs = [1]

[target_model]
exp_type = ["target"]
model_name = ["shallow"]
dataset = ["NER15"]
nn_objective = ["accuracy"]

[netflow_regression]
dataset = ["netflow_asflow"]
netflow_file_names = [["akamai-dt-handovers_1.7.17-1.8.19.csv","amazon-dt-handovers_1.7.17-1.8.19.csv","google-dt-handovers_1.7.17-1.8.19.csv"]]
input_height = [240]
steps_ahead = [4]
num_layers = [30]
start_point = [3]
jumps = [1]

[netflow_regression_daily]
mongodb_name = ["netflow_db"]
dataset = ["netflow_asflow"]
input_height = [240]
autonomous_systems = [[20940, 16509, 15169]]
date_range = ["1.7.2017-1.10.2019"]
prediction_buffer = [2]
steps_ahead = [5]
num_layers = [30]
start_hour = [15]
jumps = [24]

[netflow_plotting]
mongodb_server = ["132.72.80.61"]
dataset = ["netflow_asflow"]
models_dir = ["353_1_netflow_regression_daily"]
autonomous_systems = [[20940, 16509, 6185, 15133, 32934, 15169, 3356, 202818, 22822, 2906, 32590]]
date_range = ["1.7.2017-1.10.2019"]
as_to_test = [20940, 16509, 15169]
start_hour = [15]
input_height = [240]
prediction_buffer = [2]
steps_ahead = [5]
shuffle = [false]
jumps = [24]
model_file_name = ["best_model_1.th"]
plotting_problem = ["regression"]
k_fold = [true]
n_folds = [5]
evaluator = ["cnn"]
moving_threshold = [false, true]
max_handovers = [12]

[netflow_plotting_3_AS]
mongodb_server = ["132.72.80.61"]
dataset = ["netflow_asflow"]
models_dir = ["396_1_netflow_regression_daily"]
autonomous_systems = [[20940, 16509, 15169]]
date_range = ["1.7.2017-1.10.2019"]
as_to_test = [20940, 16509, 15169]
start_hour = [15]
input_height = [240]
prediction_buffer = [2]
steps_ahead = [5]
shuffle = [false]
jumps = [24]
model_file_name = ["best_model_1.th"]
plotting_problem = ["regression"]
k_fold = [true]
n_folds = [5]
evaluator = ["cnn"]
moving_threshold = [false, true]
max_handovers = [11]
max_final_epochs = [1]

[EEGNAS_10_layers]
dataset = ["JapaneseVowels", "Libras", "LSST", "MotorImagery", "NATOPS", "PEMS-SF", "PenDigits", "PhonemeSpectra", "RacketSports", "SelfRegulationSCP1", "SelfRegulationSCP2", "SpokenArabicDigits", "StandWalkJump", "UWaveGestureLibrary"]
iterations = [1,2,3,4,5]

[EEGNAS_10_layers_insectWingBeat]
dataset = ["InsectWingbeat"]
iterations = [1,2,3,4,5]

[EEGNAS_10_layers_EEG_1]
dataset = ["BCI_IV_2a", "BCI_IV_2b", "HG"]
iterations = [1,2,3,4,5]

[EEGNAS_10_layers_EEG_2]
dataset = ["NER15", "Opportunity", "MentalImageryLongWords"]
iterations = [1,2,3,4,5]

[EEGNAS_10_layers_RS]
dataset = ["FingerMovements", "HandMovementDirection", "Handwriting", "Heartbeat", "JapaneseVowels", "Libras", "LSST", "MotorImagery", "NATOPS", "PEMS-SF", "PenDigits", "PhonemeSpectra", "RacketSports", "SelfRegulationSCP1", "SelfRegulationSCP2", "SpokenArabicDigits", "StandWalkJump", "UWaveGestureLibrary"]
iterations = [1,2,3,4,5]
random_search = [true]

[EEGNAS_10_layers_RS_insectWingBeat]
dataset = ["InsectWingbeat"]
iterations = [1,2,3,4,5]
random_search = [true]

[EEGNAS_10_layers_RS_EEG_1]
dataset = ["BCI_IV_2a", "BCI_IV_2b", "HG"]
iterations = [1,2,3,4,5]
random_search = [true]

[EEGNAS_10_layers_RS_EEG_2]
dataset = ["NER15", "Opportunity", "MentalImageryLongWords"]
iterations = [1,2,3,4,5]
random_search = [true]

[EEGNAS_30_layers]
dataset = ["ArticularyWordRecognition", "AtrialFibrillation", "BasicMotions", "CharacterTrajectories", "Cricket", "DuckDuckGeese", "EigenWorms", "Epilepsy", "ERing", "EthanolConcentration", "FaceDetection", "FingerMovements", "HandMovementDirection", "Handwriting", "Heartbeat", "JapaneseVowels", "Libras", "LSST", "MotorImagery", "NATOPS", "PEMS-SF", "PenDigits", "PhonemeSpectra", "RacketSports", "SelfRegulationSCP1", "SelfRegulationSCP2", "SpokenArabicDigits", "StandWalkJump", "UWaveGestureLibrary"]
iterations = [1,2,3,4,5]
num_layers = [30]

[EEGNAS_30_layers_insectWingBeat]
dataset = ["InsectWingbeat"]
iterations = [1,2,3,4,5]
num_layers = [30]

[EEGNAS_30_layers_EEG_1]
dataset = ["BCI_IV_2a", "BCI_IV_2b", "HG"]
iterations = [1,2,3,4,5]
num_layers = [30]

[EEGNAS_30_layers_EEG_2]
dataset = ["NER15", "Opportunity", "MentalImageryLongWords"]
iterations = [1,2,3,4,5]
num_layers = [30]

[EEGNAS_10_layers_TF]
start_exp_idx = [7]
dataset = ["DuckDuckGeese", "EigenWorms", "Epilepsy", "ERing", "EthanolConcentration", "FaceDetection", "FingerMovements", "HandMovementDirection", "Handwriting", "Heartbeat", "JapaneseVowels", "Libras", "LSST", "MotorImagery", "NATOPS", "PEMS-SF", "PenDigits", "PhonemeSpectra", "RacketSports", "SelfRegulationSCP1", "SelfRegulationSCP2", "SpokenArabicDigits", "StandWalkJump", "UWaveGestureLibrary"]
iterations = [1,2,3,4,5]
batch_size = [8]
time_frequency = [true]

[EEGNAS_10_layers_TF_test]
dataset = ["ArticularyWordRecognition"]
iterations = [1,2,3,4,5]
batch_size = [8]
time_frequency = [true]

[EEGNAS_10_layers_TF_EEG_1]
dataset = ["BCI_IV_2a", "BCI_IV_2b", "HG"]
iterations = [1,2,3,4,5]
batch_size = [8]
time_frequency = [true]

[TS_deap]
;dataset = ["ArticularyWordRecognition", "AtrialFibrillation", "BasicMotions", "CharacterTrajectories", "Cricket", "DuckDuckGeese", "EigenWorms", "Epilepsy", "ERing", "EthanolConcentration", "FaceDetection", "FingerMovements", "HandMovementDirection", "Handwriting", "Heartbeat", "InsectWingbeat", "JapaneseVowels", "Libras", "LSST", "MotorImagery", "NATOPS", "PEMS-SF", "PenDigits", "PhonemeSpectra", "RacketSports", "SelfRegulationSCP1", "SelfRegulationSCP2", "SpokenArabicDigits", "StandWalkJump", "UWaveGestureLibrary"]
dataset = ["InsectWingbeat", "JapaneseVowels", "Libras", "LSST", "MotorImagery", "NATOPS", "PEMS-SF", "PenDigits", "PhonemeSpectra", "RacketSports", "SelfRegulationSCP1", "SelfRegulationSCP2", "SpokenArabicDigits", "StandWalkJump", "UWaveGestureLibrary"]
iterations = [1,2,3,4,5]
deap = [true]

[deap_modules]
dataset = ["BCI_IV_2a"]
deap = [true]
module_evolution = [true]
module_pop_size = [10]
module_size = [3]
evolve_modules = [false]

[BCI_IV_2b_TF_matlab]
dataset = ["BCI_IV_2b_TF"]
dataset_dir = ["export_data/BCI_IV_2b_TF_matlab"]
evaluation_metrics = [["accuracy", "kappa"]]
ga_objective = ["kappa"]
nn_objective = ["kappa"]

[NER15]
dataset = ["NER15"]
ga_objective = ["accuracy"]
nn_objective = ["accuracy"]
num_layers = [30]
